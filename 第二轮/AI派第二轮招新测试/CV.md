# CV第二轮题目  
### 出题人：赖永烨  
### 请在以下三个题目之中选择两项完成，23级同学和时间比较紧张的22级同学可以仅选择一项，有能力的同学可以全部完成。  
说明：  
- 本题目集所有题目均可采用任何人工智能工具进行辅助，但是需要提交一份关于人工智能模型的使用报告，简要说明你使用了哪些工具、怎么使用的。不得简单地将题目直接输入人工智能模型然后整理输出后上交，出题人会在面试中询问本次题目的相关信息。
- 题目大部分内容为选做，无需被吓到，整体难度适中。
- 本题目集仅限使用Python语言作答。
- 有任何不理解的地方，可以QQ私聊出题人。
- 除题目要求上交的以外，你还需要上交一个总说明文档，说明选择的题目、文件夹结构和其它需要补充的内容。题目完成了一半、没有整理好全部需要上交的内容亦可在说明文档中说明后上交。 
- 如果可以，请在9月2日前上交你的初稿（仅作检查进度用，未完成的也可以上交，以面试时的完成情况为准），上交后你依旧可以完善。你可以在群内私戳出题人然后发你的GitHub仓库链接，也可以打包成zip发送至1781056838@qq.com。上交时需提供个人姓名、年级等信息。你也可以选择不上交，待面试时展示你的全部工作。
## 题目一：  
YOLO系列一直是目标检测领域的翘楚，近年也有迅猛的发展和广阔的应用。作为一个较为新手入门的目标检测算法，本题目希望你能够熟悉YOLO的算法原理，并使用YOLO进行目标检测。  
1. 请了解YOLOv5 v8 v9 v10的算法原理，并尝试使用任意两个YOLO系列的作品在任意数据集进行目标检测。如果电脑算力有限制，可以仅使用现成模型检测而不进行训练。简要介绍你在训练与检测的过程，撰写一份文档。  
2. 小目标检测一直是CV领域的难题。请尝试使用YOLO系列作品在[北大PCB](https://robotics.pkusz.edu.cn/resources/dataset/)数据集上进行训练。出题人非常希望你可以对网络进行修改创新，当然你也可以仅做一些数据增强，但是不建议用原始数据集+未改动的YOLO完成本题。请将算法的关键点、改进点、创新点整理好写在一个文档中，便于出题人阅读你的代码、了解你的算法。  
如果你完成了这道题，可以视作你完成了第一小问的一个模型的目标检测。  
如果你用了两个不同的YOLO模型（需包含YOLOv9模型），或者你仅采用了YOLOv9模型一个模型作为基础并且有较大的创新，视作你完成了第一小问（无需提交YOLO使用心得文档）。  
如果你的电脑具有算力限制，你可以只提供代码和算法思路，而不提供实验结果，在此情况下，你仍需完成第一小问的全部内容。

你需要上交（均无需过多文字）：
- 一个说明文档，简要介绍你的完成情况  
- YOLO使用心得文档（对应第一问，尽量简洁，体现你确实做了即可）  
- 第二问的代码，需要对改动YOLO的地方进行详细注释  
- 第二问的算法介绍文档  
  
## 题目二：  
手写数字识别是一个经典的计算机视觉问题，本题目希望你能够熟悉手写数字识别的算法原理，并使用任何神经网络模型在任意手写数字识别数据集上进行评测（建议是mnist数据集+你自己扩充的数据集，必须是图片数据集）。由易到难出题人设置了以下目标：
1. 能够在你自己数据集的测试集达到50%准确率  
2. 能够在你自己的数据集的测试集达到95%准确率  
3. 能够识别你自己手写的数字  
4. 能够识别部分出题人手写的数字（白纸黑字，阿拉伯数字0-9）  
   
你需要上交：
- 一个说明文档，说明完成了哪些目标，完成情况如何（完成了前三个目标才允许说有信心完成第四个目标）
- 你的代码文件，需要详细的注释
- 一份简要的实验报告，需包含网络原理图
- 你的训练集样例（3-5张有代表性的，可以附在报告中）
- 训练的模型文件和采用该文件在出题人测试集上的测试说明（会给出一张样例），尽量一行命令可以实现测试。
  
## 题目三：
自监督学习和小样本学习是CV领域的两个分支，请选择其中一个领域，阅读至少4篇论文（其中可以包含CV的基础论文），撰写阅读笔记，需要有一定的篇幅，可以采用AI辅助阅读，但是用AI生成阅读笔记是不被允许的。若选此题，面试可能会考察你对所选论文的了解程度。
1. 自监督学习：阅读的论文必须包含下列文章：
- He, Kaiming, et al. “Masked Autoencoders Are Scalable Vision Learners.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, https://doi.org/10.1109/cvpr52688.2022.01553.  
- Wang, Haochen, et al. DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions. Sept. 2023.
2. 小样本学习：阅读的论文必须包含下列文章：
- Radford, Alec, et al. “Learning Transferable Visual Models From Natural Language Supervision.” Cornell University - arXiv,Cornell University - arXiv, Feb. 2021.

## 答题建议：
第一题适合所有同学，尤其适合有一定项目经历的同学，难度较低。  
第二题适合有一定机器学习基础的大一同学，和上过机器学习课程的22级同学。  
第三题适合有相当的CV基础，或者有丰富的论文阅读经验的同学。  
